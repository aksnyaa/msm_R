---
title: "ДЗ6_Деркач_3"
author: "Деркач Аксинья"
date: "2025-04-27"
output: html_document
---

```{r}
library(knitr)
library(kableExtra)
library(magrittr)
library(openxlsx)
library(pander)
library(rio)
library(MASS)
library(factoextra)
```

 > Провести линейный дискриминантный анализ на массиве данных об индексе качества жизни.

```{r}
df <- read.xlsx('russian_regions.xlsx', sheet = 'Data')
```

```{r, echo = FALSE}
desc <- read.xlsx('russian_regions.xlsx', sheet = 'Description of data')
kbl(desc, caption = "Таблица 1. Описание данных", booktabs = T, 
    col.names = c("Переменная", "Описание переменной")) %>% 
  kable_classic_2(html_font = "Cambria", font_size = 10, full_width = F) %>%
  pack_rows("Зависимая переменная", 1, 1) %>%
  pack_rows("Уровень жизни", 2, 3) %>%
  pack_rows("Медицина", 4, 6) %>%
  pack_rows("Экология", 7, 8) %>%
  pack_rows("Рынок труда", 9, 17) %>%
  pack_rows("Прочее", 18, 18)
```

```{r}
df1 <- df[,2:18]
```

Стандартизируем признаки:
```{r}
cluster_df <- scale(df1)
```

Метод k-means для 2х кластеров:
```{r}
kmeans3 <- kmeans(cluster_df, centers = 2)
kmeans3
```

```{r}
kmeans3$centers[1,]
```

Присоединение вектора значений принадлежности к кластеру к основным данным:
```{r}
cl <- kmeans3$cluster
cluster_df <- as.data.frame(cbind(cluster_df, cl))
```

Разделение выборки на обучающую (2/3) и тестовую (1/3):
```{r}
smpl_size <- floor(2/3 * nrow(cluster_df))

set.seed(123)
train_ind <- sample(seq_len(nrow(cluster_df)), size = smpl_size)

data.train <- as.data.frame(cluster_df[train_ind,])
data.unknown <- as.data.frame(cluster_df[-train_ind,])
```

Построение дискриминантной функции:
```{r, warning = FALSE, message = FALSE}
lda.fit <- lda(cl ~ ., data = data.train)
lda.fit
```

Класс 1 составляет 22.2% данных, а класс 2 — 77.8%.
Класс 1 характеризуется более высокими значениями доходов (AVERAGE_INCOME), занятости (EMPLOYMENT_RATE и LABOR_RATE), зарплат (FEM_WALE и MALE_WAGE), но большим гендерным разрывом (WAGE_GENDER_GAP), то есть, в основном классы отличаются по экономическим показателям.

```{r}
plot(lda.fit)
```

```{r}
lda.pred <- predict(lda.fit, data.unknown)
names(lda.pred)
```

Диаграмма рассеяния значений дискриминантных функций
```{r}
plot(lda.pred$x, lda.pred$x) # make a scatterplot
text(lda.pred$x, lda.pred$x, cl, cex = 0.7, pos = 4, col = "blue") # add labels
```

Таблица соответствия предсказанных классов исходным:

```{r}
lda.pred$class
table(lda.pred$class, data.unknown[,c("cl")])
summary(lda.pred$class)
```

Высокая точность модели: (24 + 1) / 27 = 92.6%. Модель хорошо предсказывает класс 2, но менее точно класс 1.

```{r}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.unknown[,c("cl")])
```

Точность модели=83.3%. Модель идеально предсказывает класс 2, но допускает ошибки в 33% случаев для класса 1.

Лямбда Уилкса:

```{r}
ldam <- manova(as.matrix(data.unknown) ~ lda.pred$class)
summary(ldam, test = "Wilks")
```

Значение p-value=0.0004506 мало (< 0.001), что означает, что гипотеза H_0 (=нет различий между классами) отвергается, т.е. классы статистически значимо различаются по переменным. Wilks' Lambda также близка к 0 и указывает на значительные различия между группами.


